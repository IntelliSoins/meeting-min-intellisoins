# üéôÔ∏è Strat√©gie Whisper Local Mobile - whisper.cpp

## üéØ D√©cision : Full Local avec whisper.cpp

Suite √† l'analyse des options, nous adoptons **whisper.cpp + mod√®les GGUF quantifi√©s** pour le mobile.

**Probabilit√© de succ√®s : 85% (meilleur choix)**

---

## üèÜ Pourquoi whisper.cpp ?

### Avantages Techniques

‚úÖ **Cross-Platform Unifi√©**
- M√™me codebase C++ pour iOS et Android
- M√™mes mod√®les GGUF partag√©s entre desktop et mobile
- Int√©gration Rust via FFI naturelle (d√©j√† utilis√© pour desktop)

‚úÖ **Optimisation Mobile Exceptionnelle**
- **iOS** : Support Core ML (Apple Neural Engine) ‚ö°
- **Android** : Compilation NDK optimis√©e, support NNAPI optionnel
- Quantisation int8/q5/q6 ‚Üí mod√®les compacts (40-200 MB)

‚úÖ **Performance Prouv√©e**
- tiny.en quantifi√© : **temps r√©el** sur mobile moderne
- base.en quantifi√© : **2-3x realtime** (30s audio = 10-15s processing)
- Utilis√© par de nombreuses apps production

‚úÖ **Architecture Identique au Desktop**
- Desktop utilise d√©j√† whisper-rs (wrapper de whisper.cpp)
- R√©utilisation maximale du code Rust existant
- Maintenance simplifi√©e

---

## üìä Comparaison D√©taill√©e

| Approche | Co√ªt | Performance | Offline | Complexit√© | Maintenance |
|----------|------|-------------|---------|------------|-------------|
| **whisper.cpp (local)** ‚≠ê | 0‚Ç¨ | Excellent | ‚úÖ | Moyenne | Faible |
| Cloud (Groq/Deepgram) | 0-50‚Ç¨/mois | Excellent | ‚ùå | Faible | Moyenne |
| Core ML seul (iOS) | 0‚Ç¨ | Excellent (iOS) | ‚úÖ | Moyenne | Moyenne |
| TFLite seul (Android) | 0‚Ç¨ | Bon | ‚úÖ | Moyenne | Moyenne |
| Distil-Whisper | 0‚Ç¨ | Meilleur | ‚úÖ | √âlev√©e | √âlev√©e |

---

## üèóÔ∏è Architecture Mise √† Jour

### Structure Rust (Mobile vs Desktop)

```
apps/mobile/src-tauri/src/
‚îú‚îÄ‚îÄ lib.rs
‚îú‚îÄ‚îÄ audio/
‚îÇ   ‚îú‚îÄ‚îÄ mobile.rs                  # Capture audio mobile (mic seul)
‚îÇ   ‚îî‚îÄ‚îÄ vad.rs                     # Voice Activity Detection (r√©utilis√©)
‚îÇ
‚îú‚îÄ‚îÄ whisper_engine/                # R√âUTILIS√â du desktop !
‚îÇ   ‚îú‚îÄ‚îÄ whisper_engine.rs          # Wrapper whisper.cpp
‚îÇ   ‚îú‚îÄ‚îÄ model_manager.rs           # T√©l√©chargement/cache mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ inference.rs               # Inf√©rence + streaming
‚îÇ   ‚îî‚îÄ‚îÄ mobile.rs                  # Adaptations mobile (NOUVEAU)
‚îÇ
‚îî‚îÄ‚îÄ commands.rs                    # Commandes Tauri
```

### Mod√®les Recommand√©s par Plateforme

#### iPhone (iOS)

| Mod√®le | Taille | Vitesse | Qualit√© | Cas d'usage |
|--------|--------|---------|---------|-------------|
| **tiny.en** (q8) | ~40 MB | Temps r√©el | Correct | D√©mo, prototypage |
| **base.en** (q5) | ~75 MB | 2-3x RT | Bon | **Production recommand√©** ‚≠ê |
| **small.en** (q5) | ~200 MB | 4-5x RT | Tr√®s bon | Power users, iPad |

**Configuration Core ML (optionnel) :**
- Activer `WHISPER_COREML=1` pour encoder ‚Üí 2-3x plus rapide
- iPhone 12+ recommand√© pour Core ML

#### Android

| Mod√®le | Taille | Vitesse | Qualit√© | Cas d'usage |
|--------|--------|---------|---------|-------------|
| **tiny.en** (q8) | ~40 MB | Temps r√©el | Correct | D√©mo, vieux devices |
| **base.en** (q5) | ~75 MB | 2-3x RT | Bon | **Production recommand√©** ‚≠ê |
| **base** (q5 multilang) | ~75 MB | 2-3x RT | Bon | Support FR/EN/ES |

**Optimisations Android :**
- Compilation NDK avec flags ARM Neon
- Support NNAPI optionnel (Snapdragon r√©cents)

---

## üíª Impl√©mentation Technique

### √âtape 1 : R√©utiliser whisper-rs Existant

**Le desktop utilise d√©j√† whisper-rs !**

```rust
// apps/desktop/src-tauri/Cargo.toml (EXISTANT)
[dependencies]
whisper-rs = { version = "0.13.2", features = ["raw-api", "metal"] }
```

**Pour mobile, m√™me lib avec features adapt√©es :**

```rust
// apps/mobile/src-tauri/Cargo.toml (NOUVEAU)
[dependencies]
whisper-rs = { version = "0.13.2", features = ["raw-api"] }

# iOS : ajouter "metal" feature
[target.'cfg(target_os = "ios")'.dependencies]
whisper-rs = { version = "0.13.2", features = ["raw-api", "metal"] }

# Android : features de base suffisantes
[target.'cfg(target_os = "android")'.dependencies]
whisper-rs = { version = "0.13.2", features = ["raw-api"] }
```

### √âtape 2 : Adapter le Code Existant

**Desktop (existant) :**
```rust
// apps/desktop/src-tauri/src/whisper_engine/whisper_engine.rs
pub struct WhisperEngine {
    ctx: Option<WhisperContext>,
    model_path: PathBuf,
}

impl WhisperEngine {
    pub fn load_model(&self, model_name: &str) -> Result<()> {
        // Charge depuis ~/Library/Application Support/Meetily/models/
        let model_path = get_model_path(model_name)?;
        self.ctx = Some(WhisperContext::new(&model_path)?);
    }

    pub fn transcribe(&self, audio: &[f32]) -> Result<String> {
        // Inf√©rence GPU (Metal/CUDA)
    }
}
```

**Mobile (adaptation) :**
```rust
// apps/mobile/src-tauri/src/whisper_engine/mobile.rs
use super::WhisperEngine;  // R√©utilisation !

pub struct MobileWhisperEngine {
    engine: WhisperEngine,  // Composition du desktop engine
    model_type: ModelType,  // tiny, base, small
}

impl MobileWhisperEngine {
    pub async fn new(model_type: ModelType) -> Result<Self> {
        let engine = WhisperEngine::default();

        // T√©l√©charger mod√®le si absent (premi√®re installation)
        ensure_model_downloaded(model_type).await?;

        // Charger mod√®le
        engine.load_model(&model_type.to_string())?;

        Ok(Self { engine, model_type })
    }

    pub fn transcribe_chunk(&self, audio: &[f32]) -> Result<String> {
        // D√©l√©guer √† l'engine desktop (code r√©utilis√© !)
        self.engine.transcribe(audio)
    }

    pub fn is_realtime_capable(&self) -> bool {
        // tiny.en sur mobile r√©cent = temps r√©el
        matches!(self.model_type, ModelType::TinyEn)
            && self.device_is_fast()
    }
}

#[derive(Clone, Copy)]
pub enum ModelType {
    TinyEn,   // ~40 MB, temps r√©el
    BaseEn,   // ~75 MB, 2-3x RT (recommand√©)
    SmallEn,  // ~200 MB, 4-5x RT
}
```

### √âtape 3 : T√©l√©chargement & Cache Mod√®les

**Strat√©gie :**
1. **Premi√®re installation** : t√©l√©charger mod√®le de base (base.en)
2. **Cache local** : stocker dans app data dir
3. **Mises √† jour** : v√©rifier nouvelles versions p√©riodiquement

```rust
// apps/mobile/src-tauri/src/whisper_engine/model_manager.rs
use tauri::api::path::app_data_dir;

pub async fn ensure_model_downloaded(model_type: ModelType) -> Result<PathBuf> {
    let models_dir = get_models_dir()?;  // e.g. iOS: Library/Application Support/
    let model_path = models_dir.join(model_type.filename());

    if model_path.exists() {
        return Ok(model_path);
    }

    // T√©l√©charger depuis Hugging Face ou serveur propre
    download_model(model_type, &model_path).await?;

    Ok(model_path)
}

async fn download_model(model_type: ModelType, dest: &Path) -> Result<()> {
    let url = model_type.download_url();
    // URL exemple : https://huggingface.co/.../ggml-base.en-q5_1.bin

    // T√©l√©chargement avec progress
    let client = reqwest::Client::new();
    let response = client.get(&url).send().await?;
    let total_size = response.content_length().unwrap_or(0);

    let mut file = File::create(dest)?;
    let mut downloaded = 0u64;
    let mut stream = response.bytes_stream();

    while let Some(chunk) = stream.next().await {
        let chunk = chunk?;
        file.write_all(&chunk)?;
        downloaded += chunk.len() as u64;

        // √âmettre √©v√©nement de progression
        emit_download_progress(downloaded, total_size)?;
    }

    Ok(())
}
```

### √âtape 4 : Commandes Tauri Mobile

```rust
// apps/mobile/src-tauri/src/commands.rs
#[tauri::command]
async fn start_recording_mobile(
    app: AppHandle,
    model_type: String,  // "tiny", "base", "small"
) -> Result<(), String> {
    let model = ModelType::from_str(&model_type)?;

    // 1. Initialiser Whisper engine
    let whisper = MobileWhisperEngine::new(model).await?;

    // 2. D√©marrer capture audio (mic seul)
    let audio_stream = start_microphone_capture()?;

    // 3. Traiter chunks audio en streaming
    tokio::spawn(async move {
        for audio_chunk in audio_stream {
            // VAD : ignorer silence
            if !is_speech(&audio_chunk) {
                continue;
            }

            // Transcrire
            match whisper.transcribe_chunk(&audio_chunk) {
                Ok(text) => {
                    // √âmettre vers frontend
                    app.emit("transcript-update", TranscriptUpdate {
                        text,
                        timestamp: Utc::now(),
                        is_final: true,
                    }).ok();
                }
                Err(e) => log::error!("Transcription error: {}", e),
            }
        }
    });

    Ok(())
}

#[tauri::command]
async fn download_whisper_model(
    app: AppHandle,
    model_type: String,
) -> Result<(), String> {
    let model = ModelType::from_str(&model_type)?;

    // T√©l√©charger avec events de progression
    ensure_model_downloaded(model).await
        .map_err(|e| e.to_string())?;

    Ok(())
}

#[tauri::command]
fn get_available_models() -> Vec<ModelInfo> {
    vec![
        ModelInfo {
            id: "tiny.en",
            name: "Tiny (Fast)",
            size_mb: 40,
            speed: "Real-time",
            quality: "Good",
            recommended: false,
        },
        ModelInfo {
            id: "base.en",
            name: "Base (Recommended)",
            size_mb: 75,
            speed: "2-3x real-time",
            quality: "Very Good",
            recommended: true,
        },
        ModelInfo {
            id: "small.en",
            name: "Small (Best Quality)",
            size_mb: 200,
            speed: "4-5x real-time",
            quality: "Excellent",
            recommended: false,
        },
    ]
}
```

---

## üé® UI/UX Mobile pour Whisper Local

### √âcran de Configuration Initiale

```tsx
// apps/mobile/src/screens/Setup.tsx
import { invoke } from '@tauri-apps/api/core';
import { useState } from 'react';

export function WhisperSetupScreen() {
  const [downloading, setDownloading] = useState(false);
  const [progress, setProgress] = useState(0);
  const [selectedModel, setSelectedModel] = useState('base.en');

  const models = [
    {
      id: 'tiny.en',
      name: 'Fast (40 MB)',
      description: 'Real-time transcription, good quality',
      icon: '‚ö°',
    },
    {
      id: 'base.en',
      name: 'Recommended (75 MB)',
      description: 'Best balance speed/quality',
      icon: '‚≠ê',
      recommended: true,
    },
    {
      id: 'small.en',
      name: 'Best Quality (200 MB)',
      description: 'Slower but excellent accuracy',
      icon: 'üéØ',
    },
  ];

  const handleDownload = async () => {
    setDownloading(true);

    // √âcouter √©v√©nements de progression
    await listen('download-progress', (event) => {
      setProgress(event.payload.percent);
    });

    try {
      await invoke('download_whisper_model', {
        modelType: selectedModel,
      });

      // Rediriger vers app principale
      navigate('/home');
    } catch (error) {
      alert('Download failed: ' + error);
    } finally {
      setDownloading(false);
    }
  };

  return (
    <SafeAreaView>
      <View className="p-6">
        <Text className="text-2xl font-bold mb-4">
          Choose Transcription Model
        </Text>

        <Text className="text-gray-600 mb-6">
          Meetily uses on-device AI for privacy. Choose your model:
        </Text>

        {models.map((model) => (
          <TouchableOpacity
            key={model.id}
            onPress={() => setSelectedModel(model.id)}
            className={`p-4 mb-3 rounded-xl border-2 ${
              selectedModel === model.id
                ? 'border-blue-500 bg-blue-50'
                : 'border-gray-200'
            }`}
          >
            <View className="flex-row items-center">
              <Text className="text-3xl mr-3">{model.icon}</Text>
              <View className="flex-1">
                <Text className="font-semibold">{model.name}</Text>
                <Text className="text-sm text-gray-600">
                  {model.description}
                </Text>
              </View>
              {model.recommended && (
                <Badge>Recommended</Badge>
              )}
            </View>
          </TouchableOpacity>
        ))}

        {downloading ? (
          <View className="mt-6">
            <ProgressBar progress={progress} />
            <Text className="text-center mt-2">
              Downloading model... {Math.round(progress)}%
            </Text>
          </View>
        ) : (
          <Button
            onPress={handleDownload}
            className="mt-6"
            size="lg"
          >
            Download & Continue
          </Button>
        )}

        <Text className="text-xs text-gray-500 mt-4 text-center">
          ‚úÖ 100% offline ‚Ä¢ üîí Privacy-first ‚Ä¢ üöÄ No recurring costs
        </Text>
      </View>
    </SafeAreaView>
  );
}
```

### Indicateur de Performance en Temps R√©el

```tsx
// apps/mobile/src/components/TranscriptionStatus.tsx
export function TranscriptionStatus({ isRecording }) {
  const [metrics, setMetrics] = useState({
    processingTime: 0,  // ms pour transcrire
    realtimeFactor: 1.0,  // 1.0 = temps r√©el, 2.0 = 2x plus lent
    modelType: 'base.en',
  });

  return (
    <View className="bg-gray-100 p-3 rounded-lg">
      <View className="flex-row items-center justify-between">
        <View className="flex-row items-center">
          <ActivityIndicator size="small" color="#3b82f6" />
          <Text className="ml-2 text-sm font-medium">
            {isRecording ? 'Transcribing...' : 'Ready'}
          </Text>
        </View>

        {metrics.realtimeFactor <= 1.0 ? (
          <Badge variant="success">
            ‚ö° Real-time
          </Badge>
        ) : (
          <Badge variant="warning">
            {metrics.realtimeFactor.toFixed(1)}x
          </Badge>
        )}
      </View>

      <Text className="text-xs text-gray-500 mt-1">
        Model: {metrics.modelType} ‚Ä¢ Processing: {metrics.processingTime}ms
      </Text>
    </View>
  );
}
```

---

## üì¶ Configuration Plateformes

### iOS (Info.plist)

```xml
<!-- apps/mobile/ios/Info.plist -->
<key>NSMicrophoneUsageDescription</key>
<string>Meetily needs microphone access to transcribe your meetings locally</string>

<key>UIBackgroundModes</key>
<array>
    <string>audio</string>
</array>

<!-- Core ML (optionnel, pour acc√©l√©ration) -->
<key>NSCameraUsageDescription</key>
<string>Not used, but required for Core ML access</string>
```

### Android (AndroidManifest.xml)

```xml
<!-- apps/mobile/android/AndroidManifest.xml -->
<uses-permission android:name="android.permission.RECORD_AUDIO" />
<uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />
<uses-permission android:name="android.permission.FOREGROUND_SERVICE" />

<!-- NNAPI (optionnel, pour acc√©l√©ration sur Snapdragon) -->
<uses-library android:name="com.qualcomm.qti.nnapi" android:required="false" />
```

---

## üöÄ Timeline Mise √† Jour

### Phase 1 : Monorepo (Semaine 1-2) - INCHANG√â
- Restructuration packages partag√©s

### Phase 2 : Mobile + Whisper Local (Semaine 3-6) - **√âTENDU**

**Semaine 3 : Infrastructure**
- [ ] Configurer whisper-rs pour mobile
- [ ] Adapter WhisperEngine existant
- [ ] Impl√©menter t√©l√©chargement mod√®les

**Semaine 4 : Int√©gration**
- [ ] Audio capture mobile (mic)
- [ ] Pipeline VAD + Whisper
- [ ] Tests performance iOS/Android

**Semaine 5 : UI/UX**
- [ ] √âcran setup mod√®le
- [ ] Indicateurs performance
- [ ] Gestion erreurs/fallbacks

**Semaine 6 : Optimisation**
- [ ] Core ML iOS (optionnel)
- [ ] NNAPI Android (optionnel)
- [ ] Benchmarks finaux

### Phase 3 : Features (Semaine 7-8)
- R√©sum√©s IA
- Sync cloud optionnel

### Phase 4 : Distribution (Semaine 9)
- Builds & distribution stores

**Total : 9 semaines** (au lieu de 8)

---

## üíæ Stockage & Mod√®les

### Taille Totale App

| Composant | Taille | Notes |
|-----------|--------|-------|
| App base (sans mod√®le) | ~50 MB | Code + assets |
| Mod√®le tiny.en (q8) | ~40 MB | Ultra-rapide |
| Mod√®le base.en (q5) | ~75 MB | **Recommand√©** ‚≠ê |
| Mod√®le small.en (q5) | ~200 MB | Haute qualit√© |
| **Total (base.en)** | **~125 MB** | Comparable √† apps courantes |

**Strat√©gies :**
1. **Download on first launch** (recommand√©)
   - App store = 50 MB (sans mod√®le)
   - Premier lancement : t√©l√©charge base.en (75 MB)
   - Total: 125 MB

2. **Bundle with app** (alternative)
   - App store = 125 MB (avec base.en)
   - Pr√™t imm√©diatement
   - Mais limite choix utilisateur

---

## ‚ö° Performance Attendue

### iPhone

| Mod√®le iPhone | tiny.en | base.en | small.en |
|---------------|---------|---------|----------|
| iPhone 11 | Temps r√©el | 2x RT | 5x RT |
| iPhone 12+ | Temps r√©el | 1.5x RT | 3x RT |
| iPhone 14+ (Core ML) | Temps r√©el | **Temps r√©el** ‚ö° | 2x RT |

### Android

| Device | tiny.en | base.en | small.en |
|--------|---------|---------|----------|
| Snapdragon 8 Gen 2 | Temps r√©el | 2x RT | 4x RT |
| Snapdragon 8 Gen 1 | Temps r√©el | 2.5x RT | 5x RT |
| Mid-range (SD 7) | 1.5x RT | 3x RT | 7x RT |

**L√©gende :**
- Temps r√©el = transcription aussi rapide que l'audio
- 2x RT = 30s audio = 15s processing

---

## ‚úÖ Avantages vs Cloud

| Feature | Whisper Local | Cloud (Groq/Deepgram) |
|---------|---------------|------------------------|
| **Privacy** | ‚úÖ 100% on-device | ‚ö†Ô∏è Donn√©es envoy√©es |
| **Offline** | ‚úÖ Fonctionne partout | ‚ùå N√©cessite internet |
| **Co√ªt** | ‚úÖ 0‚Ç¨ | ‚ö†Ô∏è 0-50‚Ç¨/mois |
| **Vitesse** | ‚úÖ 1-3x RT (base.en) | ‚úÖ < 1s (cloud) |
| **Qualit√©** | ‚úÖ Excellente | ‚úÖ Excellente |
| **Batterie** | ‚ö†Ô∏è Consommation CPU | ‚úÖ Minimal |
| **Setup** | ‚ö†Ô∏è T√©l√©chargement initial | ‚úÖ Imm√©diat |

**Verdict :** Whisper local est sup√©rieur pour privacy et offline, avec performances acceptables.

---

## üéØ Conclusion

### D√©cision Finale

‚úÖ **Adopter whisper.cpp + mod√®les GGUF quantifi√©s**

**Mod√®le par d√©faut :** base.en (q5) - 75 MB
**Optimisations :**
- iOS : Core ML encoder (optionnel)
- Android : NDK optimis√© ARM Neon

### R√©utilisation Code Desktop

üü¢ **90% du code Rust Whisper r√©utilis√© !**
- WhisperEngine : partag√©
- ModelManager : adapt√© pour mobile
- Inf√©rence : identique

### Next Steps

1. **Valider l'approche** ‚úÖ
2. **D√©marrer Phase 1** (monorepo)
3. **POC Whisper mobile** (Semaine 3)
4. **It√©rer & optimiser**

---

**Ce plan remplace la transcription cloud par Whisper local dans le MOBILE_MIGRATION_PLAN.md original.**
